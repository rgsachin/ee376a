{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "import numpy as np\n",
    "from sklearn.decomposition import NMF\n",
    "from keras.models import Sequential, Model, Input\n",
    "from keras.layers import Dense, Activation, Bidirectional, Add, Concatenate, Embedding, Lambda, concatenate, Dropout, TimeDistributed\n",
    "from keras.layers import LSTM, Multiply, Lambda,TimeDistributed,GRU,Multiply,Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.core import Masking\n",
    "from keras import backend as K\n",
    "from keras import losses\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from dateutil.parser import parse\n",
    "import datetime\n",
    "import copy\n",
    "from keras.layers import LeakyReLU\n",
    "import random\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "config.log_device_placement = True  # to log device placement (on which device the operation ran)\n",
    "sess = tf.Session(config=config)\n",
    "K.tensorflow_backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embed_size = 10\n",
    "hidden_size = 20\n",
    "sequential_code_channel_size=3\n",
    "logical_variables = [2,3,5,6,7]\n",
    "logical_operator = {\n",
    "    0:'not',\n",
    "    1:'',\n",
    "}\n",
    "none_var = 0\n",
    "num_vocab= len(logical_operator)+len(logical_variables)+1\n",
    "batch_size=50\n",
    "episode_len = 10\n",
    "num_epoch = 1000000\n",
    "verbose=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_log(*args):\n",
    "    if verbose:\n",
    "        print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_operations = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_length = 4\n",
    "exp = []\n",
    "\n",
    "def generate_expression(max_length,indent='-'):\n",
    "    cnf = []\n",
    "    for i in range(max_length):\n",
    "        var_1 = random.choice(logical_variables)\n",
    "        var_2 = random.choice(logical_variables)\n",
    "        var_1_not = random.choice(list(logical_operator.keys()))\n",
    "        var_2_not = random.choice(list(logical_operator.keys()))\n",
    "        cnf.append([var_1_not,var_1,var_2_not,var_2])\n",
    "    return cnf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp: [[0, 6, 0, 7], [0, 5, 0, 6], [1, 2, 0, 3], [0, 3, 0, 7], [1, 6, 1, 7]]\n"
     ]
    }
   ],
   "source": [
    "exp = generate_expression(5, '-')\n",
    "print('exp:',exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_readable_expression(cnf,idx=0):\n",
    "    exp_readable = []\n",
    "    for df in cnf:\n",
    "        disjunctions = []\n",
    "        disjunctions.append('(')\n",
    "        disjunctions.append(logical_operator[df[0]])\n",
    "        disjunctions.append(str(df[1]))\n",
    "        disjunctions.append('or')\n",
    "        disjunctions.append(logical_operator[df[2]])\n",
    "        disjunctions.append(str(df[3]))\n",
    "        disjunctions.append(')')\n",
    "        exp_readable.append(' '.join(disjunctions))\n",
    "    return ' and '.join(exp_readable).strip(),_\n",
    "\n",
    "def generate_readable_expression_(exp,idx=0):\n",
    "    org_idx=idx\n",
    "    exp_readable = []\n",
    "    if idx>=len(exp):\n",
    "        return exp_readable,idx\n",
    "    if exp[idx] in logical_operator.keys():\n",
    "        if exp[idx]==1:\n",
    "            exp_readable.append(logical_operator[exp[idx]][0])\n",
    "            exp_1,idx_end = generate_readable_expression(exp,idx+1)\n",
    "            exp_readable.extend(exp_1)\n",
    "            idx = idx_end\n",
    "        else:\n",
    "            exp_readable.append('(')\n",
    "            exp_1,idx_end = generate_readable_expression(exp,idx+1)\n",
    "            exp_readable.extend(exp_1)\n",
    "            exp_readable.append(logical_operator[exp[idx]][0])\n",
    "            exp_2,idx_end = generate_readable_expression(exp,idx_end)\n",
    "            exp_readable.extend(exp_2)\n",
    "            exp_readable.append(')')\n",
    "            idx = idx_end\n",
    "    else:\n",
    "        if exp[idx]!=0: #-check?\n",
    "            exp_readable.append(exp[idx])\n",
    "        idx+=1\n",
    "    if org_idx==0 and idx!=len(exp) and exp[idx]!=0: #-check?\n",
    "        exp_readable=[]\n",
    "    return exp_readable,idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('(  5 or  6 ) and ( not 6 or  5 ) and (  5 or  5 ) and (  5 or not 5 ) and ( not 6 or not 6 )',\n",
       " '')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_readable_expression([[1, 5, 1, 6], [0, 6, 1, 5], [1, 5, 1, 5], [1, 5, 0, 5], [0, 6, 0, 6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp: [[0, 2, 1, 3], [1, 5, 0, 2], [0, 3, 0, 2], [0, 7, 0, 2], [1, 6, 1, 2]]\n",
      "exp_readable: ( not 2 or  3 ) and (  5 or not 2 ) and ( not 3 or not 2 ) and ( not 7 or not 2 ) and (  6 or  2 )\n"
     ]
    }
   ],
   "source": [
    "verbose = True\n",
    "exp = generate_expression(5, '-')\n",
    "print('exp:',exp)\n",
    "exp_readable,_ = generate_readable_expression(exp)\n",
    "print('exp_readable:',exp_readable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simulator_generator_example(max_ip_seq_length,max_comp_seq_length):\n",
    "    batch = []\n",
    "    while True:\n",
    "        exp = generate_expression(max_ip_seq_length)\n",
    "        batch.append(exp)\n",
    "        if len(batch) % batch_size ==0 and len(batch)!=0:\n",
    "            batch = np.asarray(batch)\n",
    "            batch = [batch[:,:,0:1],batch[:,:,1:2],batch[:,:,2:3],batch[:,:,3:]]\n",
    "            yield batch\n",
    "            batch = []\n",
    "            \n",
    "def simulator_generator_example_(max_ip_seq_length,max_comp_seq_length):\n",
    "    batch = []\n",
    "    while True:\n",
    "        batch.append(generate_expression(max_ip_seq_length))\n",
    "        if len(batch) % batch_size ==0 and len(batch)!=0:\n",
    "            batch = np.expand_dims(batch,axis=-1)\n",
    "            yield batch\n",
    "            batch = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = simulator_generator_example(4,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0, 0, 0, 0]),\n",
       " array([5, 2, 6, 2]),\n",
       " array([0, 0, 0, 0]),\n",
       " array([6, 5, 3, 6])]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verbose=False\n",
    "a = gen.__next__()\n",
    "[np.asarray(z[0]).flatten() for z in a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "possible_val_combinations = [[]]\n",
    "for i in range(len(logical_variables)):\n",
    "    agumented_possible_val_combinations = []\n",
    "    for comb in possible_val_combinations:\n",
    "        agumented_possible_val_combinations.append(comb.copy()+[True])\n",
    "        agumented_possible_val_combinations.append(comb.copy()+[False])\n",
    "    possible_val_combinations=agumented_possible_val_combinations\n",
    "\n",
    "def score_function(expression1,expression2):\n",
    "    exp_1_readable,_ = generate_readable_expression(expression1)\n",
    "    exp_2_readable,_ = generate_readable_expression(expression2)\n",
    "\n",
    "    \n",
    "    print_log('exp_1_readable:',exp_1_readable)\n",
    "    print_log('exp_2_readable:',exp_2_readable)\n",
    "    \n",
    "    score = 0\n",
    "    #exp_equal = False\n",
    "    \n",
    "    for comb in possible_val_combinations:\n",
    "        val_exp_1 = exp_1_readable\n",
    "        val_exp_2 = exp_2_readable\n",
    "        for i,var in enumerate(logical_variables):\n",
    "            val_exp_1 = val_exp_1.replace(str(var),str(comb[i]))\n",
    "            val_exp_2 = val_exp_2.replace(str(var),str(comb[i]))\n",
    "            \n",
    "        print_log('val_exp_1:',val_exp_1)\n",
    "        print_log('val_exp_2:',val_exp_2)\n",
    "\n",
    "        sol1=eval(val_exp_1.strip())\n",
    "        sol2=eval(val_exp_2.strip())\n",
    "        \n",
    "        \n",
    "        exp_equal = sol1==sol2\n",
    "        #if not exp_equal:\n",
    "        #    break\n",
    "        if sol1==sol2:\n",
    "            #score=1\n",
    "            score+=1\n",
    "        else:\n",
    "            score-=1\n",
    "            #score=-1\n",
    "            #break\n",
    "    \n",
    "    print_log('sol1:',sol1)\n",
    "    print_log('sol2:',sol2)\n",
    "    # non-linear scaling\n",
    "    #score = score*np.abs(score)\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_function([[1, 3, 1, 2], [0, 3, 1, 3], [1, 2, 0, 2], [1, 3, 0, 3], [1, 3, 0, 2]],[[1, 3, 1, 2], [0, 3, 1, 3], [1, 2, 0, 2], [1, 3, 0, 3], [1, 3, 0, 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('exp_1_readable:', '(  3 or  2 ) and ( not 3 or  3 ) and (  2 or not 2 ) and (  3 or not 3 ) and (  3 or not 2 )')\n",
      "('exp_2_readable:', '(  3 or  2 ) and ( not 3 or  3 ) and (  2 or not 2 ) and (  3 or not 3 ) and ( not 3 or not 2 )')\n",
      "('val_exp_1:', '(  True or  True ) and ( not True or  True ) and (  True or not True ) and (  True or not True ) and (  True or not True )')\n",
      "('val_exp_2:', '(  True or  True ) and ( not True or  True ) and (  True or not True ) and (  True or not True ) and ( not True or not True )')\n",
      "('val_exp_1:', '(  True or  True ) and ( not True or  True ) and (  True or not True ) and (  True or not True ) and (  True or not True )')\n",
      "('val_exp_2:', '(  True or  True ) and ( not True or  True ) and (  True or not True ) and (  True or not True ) and ( not True or not True )')\n",
      "('val_exp_1:', '(  True or  True ) and ( not True or  True ) and (  True or not True ) and (  True or not True ) and (  True or not True )')\n",
      "('val_exp_2:', '(  True or  True ) and ( not True or  True ) and (  True or not True ) and (  True or not True ) and ( not True or not True )')\n",
      "('val_exp_1:', '(  True or  True ) and ( not True or  True ) and (  True or not True ) and (  True or not True ) and (  True or not True )')\n",
      "('val_exp_2:', '(  True or  True ) and ( not True or  True ) and (  True or not True ) and (  True or not True ) and ( not True or not True )')\n",
      "('val_exp_1:', '(  True or  True ) and ( not True or  True ) and (  True or not True ) and (  True or not True ) and (  True or not True )')\n",
      "('val_exp_2:', '(  True or  True ) and ( not True or  True ) and (  True or not True ) and (  True or not True ) and ( not True or not True )')\n",
      "('val_exp_1:', '(  True or  True ) and ( not True or  True ) and (  True or not True ) and (  True or not True ) and (  True or not True )')\n",
      "('val_exp_2:', '(  True or  True ) and ( not True or  True ) and (  True or not True ) and (  True or not True ) and ( not True or not True )')\n",
      "('val_exp_1:', '(  True or  True ) and ( not True or  True ) and (  True or not True ) and (  True or not True ) and (  True or not True )')\n",
      "('val_exp_2:', '(  True or  True ) and ( not True or  True ) and (  True or not True ) and (  True or not True ) and ( not True or not True )')\n",
      "('val_exp_1:', '(  True or  True ) and ( not True or  True ) and (  True or not True ) and (  True or not True ) and (  True or not True )')\n",
      "('val_exp_2:', '(  True or  True ) and ( not True or  True ) and (  True or not True ) and (  True or not True ) and ( not True or not True )')\n",
      "('val_exp_1:', '(  False or  True ) and ( not False or  False ) and (  True or not True ) and (  False or not False ) and (  False or not True )')\n",
      "('val_exp_2:', '(  False or  True ) and ( not False or  False ) and (  True or not True ) and (  False or not False ) and ( not False or not True )')\n",
      "('val_exp_1:', '(  False or  True ) and ( not False or  False ) and (  True or not True ) and (  False or not False ) and (  False or not True )')\n",
      "('val_exp_2:', '(  False or  True ) and ( not False or  False ) and (  True or not True ) and (  False or not False ) and ( not False or not True )')\n",
      "('val_exp_1:', '(  False or  True ) and ( not False or  False ) and (  True or not True ) and (  False or not False ) and (  False or not True )')\n",
      "('val_exp_2:', '(  False or  True ) and ( not False or  False ) and (  True or not True ) and (  False or not False ) and ( not False or not True )')\n",
      "('val_exp_1:', '(  False or  True ) and ( not False or  False ) and (  True or not True ) and (  False or not False ) and (  False or not True )')\n",
      "('val_exp_2:', '(  False or  True ) and ( not False or  False ) and (  True or not True ) and (  False or not False ) and ( not False or not True )')\n",
      "('val_exp_1:', '(  False or  True ) and ( not False or  False ) and (  True or not True ) and (  False or not False ) and (  False or not True )')\n",
      "('val_exp_2:', '(  False or  True ) and ( not False or  False ) and (  True or not True ) and (  False or not False ) and ( not False or not True )')\n",
      "('val_exp_1:', '(  False or  True ) and ( not False or  False ) and (  True or not True ) and (  False or not False ) and (  False or not True )')\n",
      "('val_exp_2:', '(  False or  True ) and ( not False or  False ) and (  True or not True ) and (  False or not False ) and ( not False or not True )')\n",
      "('val_exp_1:', '(  False or  True ) and ( not False or  False ) and (  True or not True ) and (  False or not False ) and (  False or not True )')\n",
      "('val_exp_2:', '(  False or  True ) and ( not False or  False ) and (  True or not True ) and (  False or not False ) and ( not False or not True )')\n",
      "('val_exp_1:', '(  False or  True ) and ( not False or  False ) and (  True or not True ) and (  False or not False ) and (  False or not True )')\n",
      "('val_exp_2:', '(  False or  True ) and ( not False or  False ) and (  True or not True ) and (  False or not False ) and ( not False or not True )')\n",
      "('val_exp_1:', '(  True or  False ) and ( not True or  True ) and (  False or not False ) and (  True or not True ) and (  True or not False )')\n",
      "('val_exp_2:', '(  True or  False ) and ( not True or  True ) and (  False or not False ) and (  True or not True ) and ( not True or not False )')\n",
      "('val_exp_1:', '(  True or  False ) and ( not True or  True ) and (  False or not False ) and (  True or not True ) and (  True or not False )')\n",
      "('val_exp_2:', '(  True or  False ) and ( not True or  True ) and (  False or not False ) and (  True or not True ) and ( not True or not False )')\n",
      "('val_exp_1:', '(  True or  False ) and ( not True or  True ) and (  False or not False ) and (  True or not True ) and (  True or not False )')\n",
      "('val_exp_2:', '(  True or  False ) and ( not True or  True ) and (  False or not False ) and (  True or not True ) and ( not True or not False )')\n",
      "('val_exp_1:', '(  True or  False ) and ( not True or  True ) and (  False or not False ) and (  True or not True ) and (  True or not False )')\n",
      "('val_exp_2:', '(  True or  False ) and ( not True or  True ) and (  False or not False ) and (  True or not True ) and ( not True or not False )')\n",
      "('val_exp_1:', '(  True or  False ) and ( not True or  True ) and (  False or not False ) and (  True or not True ) and (  True or not False )')\n",
      "('val_exp_2:', '(  True or  False ) and ( not True or  True ) and (  False or not False ) and (  True or not True ) and ( not True or not False )')\n",
      "('val_exp_1:', '(  True or  False ) and ( not True or  True ) and (  False or not False ) and (  True or not True ) and (  True or not False )')\n",
      "('val_exp_2:', '(  True or  False ) and ( not True or  True ) and (  False or not False ) and (  True or not True ) and ( not True or not False )')\n",
      "('val_exp_1:', '(  True or  False ) and ( not True or  True ) and (  False or not False ) and (  True or not True ) and (  True or not False )')\n",
      "('val_exp_2:', '(  True or  False ) and ( not True or  True ) and (  False or not False ) and (  True or not True ) and ( not True or not False )')\n",
      "('val_exp_1:', '(  True or  False ) and ( not True or  True ) and (  False or not False ) and (  True or not True ) and (  True or not False )')\n",
      "('val_exp_2:', '(  True or  False ) and ( not True or  True ) and (  False or not False ) and (  True or not True ) and ( not True or not False )')\n",
      "('val_exp_1:', '(  False or  False ) and ( not False or  False ) and (  False or not False ) and (  False or not False ) and (  False or not False )')\n",
      "('val_exp_2:', '(  False or  False ) and ( not False or  False ) and (  False or not False ) and (  False or not False ) and ( not False or not False )')\n",
      "('val_exp_1:', '(  False or  False ) and ( not False or  False ) and (  False or not False ) and (  False or not False ) and (  False or not False )')\n",
      "('val_exp_2:', '(  False or  False ) and ( not False or  False ) and (  False or not False ) and (  False or not False ) and ( not False or not False )')\n",
      "('val_exp_1:', '(  False or  False ) and ( not False or  False ) and (  False or not False ) and (  False or not False ) and (  False or not False )')\n",
      "('val_exp_2:', '(  False or  False ) and ( not False or  False ) and (  False or not False ) and (  False or not False ) and ( not False or not False )')\n",
      "('val_exp_1:', '(  False or  False ) and ( not False or  False ) and (  False or not False ) and (  False or not False ) and (  False or not False )')\n",
      "('val_exp_2:', '(  False or  False ) and ( not False or  False ) and (  False or not False ) and (  False or not False ) and ( not False or not False )')\n",
      "('val_exp_1:', '(  False or  False ) and ( not False or  False ) and (  False or not False ) and (  False or not False ) and (  False or not False )')\n",
      "('val_exp_2:', '(  False or  False ) and ( not False or  False ) and (  False or not False ) and (  False or not False ) and ( not False or not False )')\n",
      "('val_exp_1:', '(  False or  False ) and ( not False or  False ) and (  False or not False ) and (  False or not False ) and (  False or not False )')\n",
      "('val_exp_2:', '(  False or  False ) and ( not False or  False ) and (  False or not False ) and (  False or not False ) and ( not False or not False )')\n",
      "('val_exp_1:', '(  False or  False ) and ( not False or  False ) and (  False or not False ) and (  False or not False ) and (  False or not False )')\n",
      "('val_exp_2:', '(  False or  False ) and ( not False or  False ) and (  False or not False ) and (  False or not False ) and ( not False or not False )')\n",
      "('val_exp_1:', '(  False or  False ) and ( not False or  False ) and (  False or not False ) and (  False or not False ) and (  False or not False )')\n",
      "('val_exp_2:', '(  False or  False ) and ( not False or  False ) and (  False or not False ) and (  False or not False ) and ( not False or not False )')\n",
      "('sol1:', False)\n",
      "('sol2:', False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verbose=True\n",
    "score_function([[1, 3, 1, 2], [0, 3, 1, 3], [1, 2, 0, 2], [1, 3, 0, 3], [1, 3, 0, 2]],[[1, 3, 1, 2], [0, 3, 1, 3], [1, 2, 0, 2], [1, 3, 0, 3], [0, 3, 0, 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_encoder_agent():\n",
    "    x_i_not_1 = Input(shape=(None,1))\n",
    "    x_i_var_1 = Input(shape=(None,1))\n",
    "    x_i_not_2 = Input(shape=(None,1))\n",
    "    x_i_var_2 = Input(shape=(None,1))\n",
    "    \n",
    "    x_o = Input(shape=(None,1))\n",
    "    \n",
    "    var_emebed = Embedding(num_vocab, embed_size)\n",
    "\n",
    "    y_var_1 = TimeDistributed(var_emebed)(x_i_var_1)\n",
    "    y_var_1 = Lambda(lambda z:K.squeeze(z,axis=-2))(y_var_1)\n",
    "    \n",
    "    y_var_2 = TimeDistributed(var_emebed)(x_i_var_2)\n",
    "    y_var_2 = Lambda(lambda z:K.squeeze(z,axis=-2))(y_var_2)\n",
    "    \n",
    "    \n",
    "    print('x_i_not_1:',K.int_shape(x_i_not_1))\n",
    "    print('y_var_1:',K.int_shape(y_var_1))\n",
    "    print('x_i_not_1:',K.int_shape(x_i_not_2))\n",
    "    print('y_var_2:',K.int_shape(y_var_2))\n",
    "    \n",
    "    y = Concatenate(axis=-1)([x_i_not_1,y_var_1,x_i_not_2,y_var_2])\n",
    "    \n",
    "    #y = LSTM(hidden_size,return_sequences=True)(y)\n",
    "    y,h,c = LSTM(hidden_size,return_state=True)(y)\n",
    "    \n",
    "    print('x_o:',K.int_shape(x_o))\n",
    "    y_o=TimeDistributed(Embedding(sequential_code_channel_size, embed_size))(x_o)\n",
    "    y_o = Lambda(lambda z:K.squeeze(z,axis=-2))(y_o)\n",
    "    print('y_o:',K.int_shape(y_o))\n",
    "    \n",
    "    y,_,_ = LSTM(hidden_size,return_sequences=True, return_state=True)(y_o,initial_state=[h,c])\n",
    "    #y = LSTM(hidden_size,return_sequences=True)(y)\n",
    "    \n",
    "    #y = TimeDistributed(Dense(sequential_code_channel_size,activation='softmax'))(y)\n",
    "    y = TimeDistributed(Dense(sequential_code_channel_size,activation=None))(y)\n",
    "    \n",
    "    \n",
    "    y = TimeDistributed(Activation('softmax'))(y)\n",
    "    \n",
    "    model = Model(inputs=[x_i_not_1,x_i_var_1,x_i_not_2,x_i_var_2,x_o],outputs=y)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_decoder_agent():\n",
    "    x_i = Input(shape=(None,sequential_code_channel_size))\n",
    "    x_o_not_1 = Input(shape=(None,1))\n",
    "    x_o_var_1 = Input(shape=(None,1))\n",
    "    x_o_not_2 = Input(shape=(None,1))\n",
    "    x_o_var_2 = Input(shape=(None,1))\n",
    "    \n",
    "    y = TimeDistributed(Dense(embed_size))(x_i) #Embedding layer\n",
    "    \n",
    "    print('emebd:',K.int_shape(y))\n",
    "    \n",
    "    #y = Lambda(lambda z:K.squeeze(z,axis=-2))(y)\n",
    "    #y = LSTM(hidden_size,return_sequences=True)(y)\n",
    "    y,h,c = LSTM(hidden_size,return_state=True)(y)\n",
    "    \n",
    "    \n",
    "    ##--------------seq-inp-starts-----------------\n",
    "    var_emebed = Embedding(num_vocab, embed_size)\n",
    "    \n",
    "    y_var_1 = TimeDistributed(var_emebed)(x_o_var_1)\n",
    "    y_var_1 = Lambda(lambda z:K.squeeze(z,axis=-2))(y_var_1)\n",
    "    \n",
    "    y_var_2 = TimeDistributed(var_emebed)(x_o_var_2)\n",
    "    y_var_2 = Lambda(lambda z:K.squeeze(z,axis=-2))(y_var_2)\n",
    "    \n",
    "    print('x_i_not_1:',K.int_shape(x_o_not_1))\n",
    "    print('y_var_1:',K.int_shape(y_var_1))\n",
    "    print('x_i_not_1:',K.int_shape(x_o_not_2))\n",
    "    print('y_var_2:',K.int_shape(y_var_2))\n",
    "    \n",
    "    y_o = Concatenate(axis=-1)([x_o_not_1,y_var_1,x_o_not_2,y_var_2])\n",
    "    \n",
    "    print('y_o:',K.int_shape(y_o))\n",
    "    ##--------------seq-inp-end-----------------\n",
    "\n",
    "    \n",
    "    y,_,_ = LSTM(hidden_size,return_sequences=True, return_state=True)(y_o,initial_state=[h,c])\n",
    "    #y = LSTM(hidden_size,return_sequences=True)(y)\n",
    "    \n",
    "    y_not_1 = TimeDistributed(Dense(1,activation=None))(y)\n",
    "    y_not_2 = TimeDistributed(Dense(1,activation=None))(y)\n",
    "    \n",
    "    y_var_1 = TimeDistributed(Dense(num_vocab,activation=None))(y)\n",
    "    y_var_2 = TimeDistributed(Dense(num_vocab,activation=None))(y)\n",
    "    \n",
    "    y_not_1_activated = TimeDistributed(Activation('sigmoid'))(y_not_1)\n",
    "    y_not_2_activated = TimeDistributed(Activation('sigmoid'))(y_not_2)\n",
    "    \n",
    "    y_var_1_activated = TimeDistributed(Activation('softmax'))(y_var_1)\n",
    "    y_var_2_activated = TimeDistributed(Activation('softmax'))(y_var_2)\n",
    "    \n",
    "    model = Model(inputs=[x_i,\n",
    "                          x_o_not_1,\n",
    "                          x_o_var_1,\n",
    "                          x_o_not_2,\n",
    "                          x_o_var_2],\n",
    "                  outputs=[y_not_1,\n",
    "                           y_var_1,\n",
    "                           y_not_2,\n",
    "                           y_var_2])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_i_not_1: (None, None, 1)\n",
      "y_var_1: (None, None, 10)\n",
      "x_i_not_1: (None, None, 1)\n",
      "y_var_2: (None, None, 10)\n",
      "x_o: (None, None, 1)\n",
      "y_o: (None, None, 10)\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_2 (InputLayer)             (None, None, 1)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_4 (InputLayer)             (None, None, 1)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistribu (None, None, 1, 10)   80          input_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistribu (None, None, 1, 10)   80          input_4[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "input_5 (InputLayer)             (None, None, 1)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_1 (InputLayer)             (None, None, 1)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)                (None, None, 10)      0           time_distributed_1[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "input_3 (InputLayer)             (None, None, 1)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)                (None, None, 10)      0           time_distributed_2[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_3 (TimeDistribu (None, None, 1, 10)   30          input_5[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, None, 22)      0           input_1[0][0]                    \n",
      "                                                                   lambda_1[0][0]                   \n",
      "                                                                   input_3[0][0]                    \n",
      "                                                                   lambda_2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)                (None, None, 10)      0           time_distributed_3[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                    [(None, 20), (None, 2 3440        concatenate_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                    [(None, None, 20), (N 2480        lambda_3[0][0]                   \n",
      "                                                                   lstm_1[0][1]                     \n",
      "                                                                   lstm_1[0][2]                     \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_4 (TimeDistribu (None, None, 3)       63          lstm_2[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_5 (TimeDistribu (None, None, 3)       0           time_distributed_4[0][0]         \n",
      "====================================================================================================\n",
      "Total params: 6,093\n",
      "Trainable params: 6,093\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_agent = get_encoder_agent()\n",
    "encoder_agent.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emebd: (None, None, 10)\n",
      "x_i_not_1: (None, None, 1)\n",
      "y_var_1: (None, None, 10)\n",
      "x_i_not_1: (None, None, 1)\n",
      "y_var_2: (None, None, 10)\n",
      "y_o: (None, None, 22)\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_8 (InputLayer)             (None, None, 1)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_10 (InputLayer)            (None, None, 1)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_7 (TimeDistribu (None, None, 1, 10)   80          input_8[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_8 (TimeDistribu (None, None, 1, 10)   80          input_10[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "input_6 (InputLayer)             (None, None, 3)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_7 (InputLayer)             (None, None, 1)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)                (None, None, 10)      0           time_distributed_7[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "input_9 (InputLayer)             (None, None, 1)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)                (None, None, 10)      0           time_distributed_8[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_6 (TimeDistribu (None, None, 10)      40          input_6[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)      (None, None, 22)      0           input_7[0][0]                    \n",
      "                                                                   lambda_4[0][0]                   \n",
      "                                                                   input_9[0][0]                    \n",
      "                                                                   lambda_5[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                    [(None, 20), (None, 2 2480        time_distributed_6[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                    [(None, None, 20), (N 3440        concatenate_2[0][0]              \n",
      "                                                                   lstm_3[0][1]                     \n",
      "                                                                   lstm_3[0][2]                     \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_9 (TimeDistribu (None, None, 1)       21          lstm_4[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_11 (TimeDistrib (None, None, 8)       168         lstm_4[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_10 (TimeDistrib (None, None, 1)       21          lstm_4[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_12 (TimeDistrib (None, None, 8)       168         lstm_4[0][0]                     \n",
      "====================================================================================================\n",
      "Total params: 6,418\n",
      "Trainable params: 6,418\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder_agent = get_decoder_agent()\n",
    "decoder_agent.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_agent.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "encoder_agent.compile(optimizer='adam', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import TensorBoard,ReduceLROnPlateau\n",
    "\n",
    "tb_callback = TensorBoard(log_dir='./logs', histogram_freq=0, batch_size=32, write_graph=True, write_grads=False, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sparse_to_dense(sparse,reward,num_dims):\n",
    "    dense = np.zeros((sparse.shape[0],sparse.shape[1],num_dims))\n",
    "    for i in range(sparse.shape[0]):\n",
    "        for j in range(sparse.shape[1]):\n",
    "            #if sparse[i,j,0] == 0: #-check?\n",
    "            #    break\n",
    "            dense[i,j,sparse[i,j,0]]=reward[i]\n",
    "    return dense\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0., -1.],\n",
       "        [-1.,  0.]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_to_dense(np.asarray([[[1],[0]]]),[-1,1],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_ip_seq_length = 4\n",
    "max_seq_communication_length = 10\n",
    "max_op_seq_length = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!rm -rf poorboard_full_diff_CNF.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f= open(\"poorboard_full_diff_CNF.txt\",\"w+\")\n",
    "def print_board(*args):\n",
    "    f.write(','.join([str(z) for z in args])+'\\n')\n",
    "    f.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_reparam: (None, None, 3)\n",
      "y: [<tf.Tensor 'model_2/time_distributed_9/Reshape_1:0' shape=(?, ?, 1) dtype=float32>, <tf.Tensor 'model_2/time_distributed_11/Reshape_1:0' shape=(?, ?, 8) dtype=float32>, <tf.Tensor 'model_2/time_distributed_10/Reshape_1:0' shape=(?, ?, 1) dtype=float32>, <tf.Tensor 'model_2/time_distributed_12/Reshape_1:0' shape=(?, ?, 8) dtype=float32>]\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_11 (InputLayer)            (None, None, 1)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_12 (InputLayer)            (None, None, 1)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_13 (InputLayer)            (None, None, 1)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_14 (InputLayer)            (None, None, 1)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_15 (InputLayer)            (None, None, 1)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "model_1 (Model)                  (None, None, 3)       6173        input_11[0][0]                   \n",
      "                                                                   input_12[0][0]                   \n",
      "                                                                   input_13[0][0]                   \n",
      "                                                                   input_14[0][0]                   \n",
      "                                                                   input_15[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "input_17 (InputLayer)            (None, None, 1)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_18 (InputLayer)            (None, None, 1)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_19 (InputLayer)            (None, None, 1)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_20 (InputLayer)            (None, None, 1)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "model_2 (Model)                  [(None, None, 1), (No 6498        model_1[1][0]                    \n",
      "                                                                   input_17[0][0]                   \n",
      "                                                                   input_18[0][0]                   \n",
      "                                                                   input_19[0][0]                   \n",
      "                                                                   input_20[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 12,511\n",
      "Trainable params: 12,511\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_inp_not_1 = Input(shape=(None,1))\n",
    "encoder_inp_var_1 = Input(shape=(None,1))\n",
    "encoder_inp_not_2 = Input(shape=(None,1))\n",
    "encoder_inp_var_2 = Input(shape=(None,1))\n",
    "encoder_seq_gen_inp = Input(shape=(None,1))\n",
    "sample_inp = Input(shape=(None,sequential_code_channel_size))\n",
    "decoder_seq_gen_inp_not_1 = Input(shape=(None,1))\n",
    "decoder_seq_gen_inp_var_1 = Input(shape=(None,1))\n",
    "decoder_seq_gen_inp_not_2 = Input(shape=(None,1))\n",
    "decoder_seq_gen_inp_var_2 = Input(shape=(None,1))\n",
    "\n",
    "\n",
    "y_encoder_pred = encoder_agent([encoder_inp_not_1,\n",
    "                                encoder_inp_var_1,\n",
    "                                encoder_inp_not_2,\n",
    "                                encoder_inp_var_2,\n",
    "                                encoder_seq_gen_inp])\n",
    "\n",
    "#y_reparam = Multiply()([y_encoder_pred,sample_inp])\n",
    "y_reparam = y_encoder_pred\n",
    "\n",
    "#y_reparam = y_encoder_pred\n",
    "\n",
    "print('y_reparam:',K.int_shape(y_reparam))\n",
    "\n",
    "#print('decoder_seq_gen_inp:',K.int_shape(decoder_seq_gen_inp),decoder_seq_gen_inp)\n",
    "y = decoder_agent([y_reparam,\n",
    "                   decoder_seq_gen_inp_not_1,\n",
    "                  decoder_seq_gen_inp_var_1,\n",
    "                  decoder_seq_gen_inp_not_2,\n",
    "                  decoder_seq_gen_inp_var_2])\n",
    "print('y:',y)\n",
    "end_end_model = Model([encoder_inp_not_1,\n",
    "                       encoder_inp_var_1,\n",
    "                       encoder_inp_not_2,\n",
    "                       encoder_inp_var_2,\n",
    "                       encoder_seq_gen_inp,\n",
    "                       sample_inp,\n",
    "                       decoder_seq_gen_inp_not_1,\n",
    "                       decoder_seq_gen_inp_var_1,\n",
    "                      decoder_seq_gen_inp_not_2,\n",
    "                      decoder_seq_gen_inp_var_2],y)\n",
    "\n",
    "end_end_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "end_end_model.compile(optimizer='adam', loss=['binary_crossentropy',\n",
    "                                              'categorical_crossentropy',\n",
    "                                              'binary_crossentropy',\n",
    "                                              'categorical_crossentropy'\n",
    "                                             ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_each_step_encode(agent,inp,max_seq_len):\n",
    "    seq_gen_inp_initial=np.zeros((batch_size,1,1))\n",
    "    seq_gen_inp = seq_gen_inp_initial.copy()\n",
    "    for p in range(0,max_seq_len):   \n",
    "        #print('encoder_seq_gen_inp:',encoder_seq_gen_inp.shape)\n",
    "        inp_agumented = inp.copy()\n",
    "        inp_agumented.append(seq_gen_inp)\n",
    "        \n",
    "        output = agent.predict(inp_agumented)\n",
    "        \n",
    "        \n",
    "        \n",
    "        output_disctretised = np.argmax(output,axis=-1)\n",
    "        output_disctretised=np.expand_dims(output_disctretised,axis=-1)\n",
    "        seq_gen_inp = np.concatenate((seq_gen_inp_initial, output_disctretised), axis=-2)\n",
    "    seq_gen_inp=seq_gen_inp[:,:-1,:]\n",
    "    return seq_gen_inp,output,output_disctretised\n",
    "\n",
    "\n",
    "def predict_each_step_decode(agent,inp,max_seq_len):\n",
    "    seq_gen_inp_initial=[np.zeros((batch_size,1,1)) for z in range(4)] #-check?\n",
    "    seq_gen_inp = seq_gen_inp_initial.copy()\n",
    "    for p in range(0,max_seq_len):   \n",
    "        #print('encoder_seq_gen_inp:',encoder_seq_gen_inp.shape)\n",
    "        inp_agumented = inp.copy()\n",
    "        inp_agumented.extend(seq_gen_inp)\n",
    "        \n",
    "        #print('inp:',[z.shape for z in inp_agumented])\n",
    "        \n",
    "        output = agent.predict(inp_agumented)\n",
    "        \n",
    "        #print('output:',[z.shape for z in output])\n",
    "        \n",
    "        \n",
    "        output_disctretised = [np.expand_dims(np.argmax(z,axis=-1),axis=-1) for z in output]\n",
    "        \n",
    "        #print('output_disctretised:',[z.shape for z in output_disctretised])\n",
    "        \n",
    "        seq_gen_inp = np.concatenate((seq_gen_inp_initial, output_disctretised), axis=-2)\n",
    "    seq_gen_inp=[z[:,:-1,:] for z in seq_gen_inp]\n",
    "    #print('#_# seq_gen_inp:',[z.shape for z in seq_gen_inp])\n",
    "    return seq_gen_inp,output,output_disctretised\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "verbose = 0\n",
    "batch_size=50\n",
    "sim_gen = simulator_generator_example(max_ip_seq_length,max_seq_communication_length)\n",
    "num_epoch_steps=1\n",
    "\n",
    "for epoch_num in range(0,num_epoch,num_epoch_steps):\n",
    "    encoder_inp = sim_gen.__next__()\n",
    "    \n",
    "    encoder_seq_gen_inp,encoder_output,encoder_output_disctretised=predict_each_step_encode(encoder_agent,encoder_inp,max_seq_communication_length)\n",
    "    decoder_seq_gen_inp,decoder_output,decoder_output_disctretised=predict_each_step_decode(decoder_agent,[encoder_output],max_op_seq_length)\n",
    "\n",
    "    \n",
    "    #print('encoder_input:',encoder_inp[0].flatten())\n",
    "    #print('encoder_seq_gen_inp:',encoder_seq_gen_inp[0].flatten())\n",
    "    #print('encoder_output:',encoder_output_disctretised[0].flatten())\n",
    "    #print('decoder_output:',decoder_output_disctretised[0].flatten())\n",
    "    \n",
    "    #print('encoder_inp:',np.asarray(encoder_inp).shape)\n",
    "    #print('encoder_inp:',[z.shape for z in encoder_inp])\n",
    "    \n",
    "    #print('=>decoder_output_disctretised:',np.asarray(decoder_output_disctretised).shape)\n",
    "    \n",
    "    batch_inp = np.squeeze(encoder_inp,axis=-1)\n",
    "    batch_inp = np.einsum('fbl->blf',batch_inp)\n",
    "    batch_op = np.squeeze(decoder_output_disctretised,axis=-1)\n",
    "    batch_op = np.einsum('fbl->blf',batch_op)\n",
    "    \n",
    "    #print('batch_inp:',batch_inp.shape)\n",
    "    #print('batch_op:',batch_op.shape)\n",
    "    \n",
    "    rewards = []\n",
    "    for idx,(ip_exp,op_exp) in enumerate(zip(batch_inp,batch_op)):\n",
    "    #for idx in range(batch_size):\n",
    "        try:\n",
    "            #print('ip_exp:',ip_exp.shape)\n",
    "            #print('op_exp:',ip_exp.shape)\n",
    "            \n",
    "            reward = score_function(ip_exp,op_exp)\n",
    "        except Exception as e:\n",
    "            print('T_T ',e,':',ip_exp,':',op_exp)\n",
    "            reward = -1000\n",
    "        rewards.append(reward)\n",
    "    \n",
    "    #print('input:',ip_exp)\n",
    "    #print('output:',op_exp)\n",
    "    \n",
    "    #print('rewards:',np.asarray(rewards).shape)\n",
    "    \n",
    "    print_board(np.sum(rewards),np.mean(rewards))\n",
    "    \n",
    "    def sparse_to_dense(sparse,reward,num_dims):\n",
    "        dense = np.zeros((sparse.shape[0],sparse.shape[1],num_dims))\n",
    "        for i in range(sparse.shape[0]):\n",
    "            for j in range(sparse.shape[1]):\n",
    "                #if sparse[i,j,0] == 0: #-check?\n",
    "                #    break\n",
    "                dense[i,j,sparse[i,j,0]]=reward[i]\n",
    "        return dense\n",
    "    \n",
    "    #encoder_reward = sparse_to_dense(encoder_output_disctretised,rewards,sequential_code_channel_size)\n",
    "    \n",
    "    decoder_reward=[]\n",
    "    decoder_reward.append( np.asarray([u*v for u,v in zip(decoder_output_disctretised[0],rewards)]))\n",
    "    decoder_reward.append(sparse_to_dense(np.expand_dims(decoder_output_disctretised[1],axis=-1),rewards,num_vocab))\n",
    "    decoder_reward.append( np.asarray([u*v for u,v in zip(decoder_output_disctretised[2],rewards)]))\n",
    "    decoder_reward.append(sparse_to_dense(np.expand_dims(decoder_output_disctretised[3],axis=-1),rewards,num_vocab))\n",
    "    \n",
    "    #print('decoder_reward:',[z.shape for z in decoder_reward])\n",
    "    sampling_input = sparse_to_dense(encoder_output_disctretised,np.ones(batch_size),sequential_code_channel_size)\n",
    "\n",
    "    \n",
    "#     import collections\n",
    "#     print('decoder_reward:',collections.Counter(decoder_reward.flatten()))\n",
    "\n",
    "#     import pdb; pdb.set_trace()\n",
    "    #print('encoder_inp:',encoder_inp.shape)\n",
    "    #print('encoder_seq_gen_inp:',encoder_seq_gen_inp.shape)\n",
    "    #print('sampling_input:',sampling_input.shape)\n",
    "    #print('decoder_seq_gen_inp:',decoder_seq_gen_inp.shape)\n",
    "    \n",
    "    #print('encoder_inp:',[z.shape for z in encoder_inp])\n",
    "    #print('encoder_seq_gen_inp:',encoder_seq_gen_inp.shape)\n",
    "    #print('sampling_input:',sampling_input.shape)\n",
    "    #print('decoder_seq_gen_inp:',[z.shape for z in decoder_seq_gen_inp])\n",
    "    \n",
    "    \n",
    "       \n",
    "    end_end_model.fit([encoder_inp[0],\n",
    "                       encoder_inp[1],\n",
    "                       encoder_inp[2],\n",
    "                       encoder_inp[3],\n",
    "                       encoder_seq_gen_inp,\n",
    "                       sampling_input,\n",
    "                       decoder_seq_gen_inp[0],\n",
    "                      decoder_seq_gen_inp[1],\n",
    "                      decoder_seq_gen_inp[2],\n",
    "                      decoder_seq_gen_inp[3]],\n",
    "                      \n",
    "                      decoder_reward,\n",
    "                      initial_epoch = num_epoch,\n",
    "                      epochs=num_epoch+num_epoch_steps,\n",
    "                      #callbacks=[tb_callback],\n",
    "                      verbose=verbose)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('( not 2 or not 2 ) and ( not 3 or  3 ) and (  2 or not 2 ) and (  3 or  3 )', array([[[ 0., -1.],\n",
      "        [-1.,  0.]]]))\n",
      "('(  3 or not 2 ) and ( not 3 or not 3 ) and (  3 or not 3 ) and (  3 or not 2 )', array([[[ 0., -1.],\n",
      "        [-1.,  0.]]]))\n"
     ]
    }
   ],
   "source": [
    "print(generate_readable_expression(batch_inp[0]))\n",
    "print(generate_readable_expression(batch_inp[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(encoder_output[0],axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.307485  , 0.28382766, 0.40868732],\n",
       "       [0.31341413, 0.28543827, 0.40114754],\n",
       "       [0.31783542, 0.28850314, 0.39366144],\n",
       "       [0.32115772, 0.29198703, 0.38685524],\n",
       "       [0.32366627, 0.2953838 , 0.38094994],\n",
       "       [0.32558048, 0.29844677, 0.37597272],\n",
       "       [0.3270603 , 0.30108273, 0.37185696],\n",
       "       [0.32821897, 0.30328336, 0.36849773],\n",
       "       [0.32913572, 0.30508313, 0.36578116],\n",
       "       [0.32986656, 0.30653438, 0.3635991 ]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_output[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampling_input[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('decoder_output:',decoder_output_disctretised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\r\n",
      "#\r\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\r\n",
      "# you may not use this file except in compliance with the License.\r\n",
      "# You may obtain a copy of the License at\r\n",
      "#\r\n",
      "#     http://www.apache.org/licenses/LICENSE-2.0\r\n",
      "#\r\n",
      "# Unless required by applicable law or agreed to in writing, software\r\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n",
      "# See the License for the specific language governing permissions and\r\n",
      "# limitations under the License.\r\n",
      "# ==============================================================================\r\n",
      "\r\n",
      "\"\"\"Utilities to create TensorProtos.\"\"\"\r\n",
      "from __future__ import absolute_import\r\n",
      "from __future__ import division\r\n",
      "from __future__ import print_function\r\n",
      "\r\n",
      "import numpy as np\r\n",
      "import six\r\n",
      "\r\n",
      "from tensorflow.core.framework import tensor_pb2\r\n",
      "from tensorflow.core.framework import tensor_shape_pb2\r\n",
      "from tensorflow.python.framework import ops\r\n",
      "from tensorflow.python.framework import tensor_shape\r\n",
      "from tensorflow.python.util import compat\r\n",
      "\r\n",
      "# Fallback in case fast_tensor_util is not properly compiled.\r\n",
      "# pylint: disable=g-import-not-at-top\r\n",
      "try:\r\n",
      "  from tensorflow.python.framework import fast_tensor_util\r\n",
      "  _FAST_TENSOR_UTIL_AVAILABLE = True\r\n",
      "except ImportError:\r\n",
      "  _FAST_TENSOR_UTIL_AVAILABLE = False\r\n",
      "\r\n",
      "from tensorflow.python.framework import dtypes\r\n",
      "from tensorflow.python.framework import ops\r\n",
      "# pylint: enable=g-import-not-at-top\r\n",
      "\r\n",
      "\r\n",
      "def ExtractBitsFromFloat16(x):\r\n",
      "  return np.asscalar(np.asarray(x, dtype=np.float16).view(np.uint16))\r\n",
      "\r\n",
      "\r\n",
      "def SlowAppendFloat16ArrayToTensorProto(tensor_proto, proto_values):\r\n",
      "  tensor_proto.half_val.extend([\r\n",
      "      ExtractBitsFromFloat16(x) for x in proto_values])\r\n",
      "\r\n",
      "if _FAST_TENSOR_UTIL_AVAILABLE:\r\n",
      "  _NP_TO_APPEND_FN = {\r\n",
      "      # TODO(sesse): We should have a\r\n",
      "      # fast_tensor_util.AppendFloat16ArrayToTensorProto,\r\n",
      "      # but it seems np.float16_t doesn't exist?\r\n",
      "      np.float16: SlowAppendFloat16ArrayToTensorProto,\r\n",
      "      np.float32: fast_tensor_util.AppendFloat32ArrayToTensorProto,\r\n",
      "      np.float64: fast_tensor_util.AppendFloat64ArrayToTensorProto,\r\n",
      "      np.int32: fast_tensor_util.AppendInt32ArrayToTensorProto,\r\n",
      "      np.int64: fast_tensor_util.AppendInt64ArrayToTensorProto,\r\n",
      "      np.uint8: fast_tensor_util.AppendUInt8ArrayToTensorProto,\r\n",
      "      np.uint16: fast_tensor_util.AppendUInt16ArrayToTensorProto,\r\n",
      "      np.int8: fast_tensor_util.AppendInt8ArrayToTensorProto,\r\n",
      "      np.int16: fast_tensor_util.AppendInt16ArrayToTensorProto,\r\n",
      "      np.complex64: fast_tensor_util.AppendComplex64ArrayToTensorProto,\r\n",
      "      np.complex128: fast_tensor_util.AppendComplex128ArrayToTensorProto,\r\n",
      "      np.object: fast_tensor_util.AppendObjectArrayToTensorProto,\r\n",
      "      np.bool: fast_tensor_util.AppendBoolArrayToTensorProto,\r\n",
      "      dtypes.qint8.as_numpy_dtype:\r\n",
      "          fast_tensor_util.AppendInt8ArrayToTensorProto,\r\n",
      "      dtypes.quint8.as_numpy_dtype:\r\n",
      "          fast_tensor_util.AppendUInt8ArrayToTensorProto,\r\n",
      "      dtypes.qint16.as_numpy_dtype:\r\n",
      "          fast_tensor_util.AppendInt8ArrayToTensorProto,\r\n",
      "      dtypes.quint16.as_numpy_dtype:\r\n",
      "          fast_tensor_util.AppendUInt8ArrayToTensorProto,\r\n",
      "      dtypes.qint32.as_numpy_dtype:\r\n",
      "          fast_tensor_util.AppendInt32ArrayToTensorProto,\r\n",
      "      # NOTE(touts): Intentionally no way to feed a DT_BFLOAT16.\r\n",
      "  }\r\n",
      "else:\r\n",
      "\r\n",
      "  def SlowAppendFloat32ArrayToTensorProto(tensor_proto, proto_values):\r\n",
      "    tensor_proto.float_val.extend([np.asscalar(x) for x in proto_values])\r\n",
      "\r\n",
      "  def SlowAppendFloat64ArrayToTensorProto(tensor_proto, proto_values):\r\n",
      "    tensor_proto.double_val.extend([np.asscalar(x) for x in proto_values])\r\n",
      "\r\n",
      "  def SlowAppendIntArrayToTensorProto(tensor_proto, proto_values):\r\n",
      "    tensor_proto.int_val.extend([np.asscalar(x) for x in proto_values])\r\n",
      "\r\n",
      "  def SlowAppendQIntArrayToTensorProto(tensor_proto, proto_values):\r\n",
      "    tensor_proto.int_val.extend([np.asscalar(x[0]) for x in proto_values])\r\n",
      "\r\n",
      "  def SlowAppendInt64ArrayToTensorProto(tensor_proto, proto_values):\r\n",
      "    tensor_proto.int64_val.extend([np.asscalar(x) for x in proto_values])\r\n",
      "\r\n",
      "  def SlowAppendComplex64ArrayToTensorProto(tensor_proto, proto_values):\r\n",
      "    tensor_proto.scomplex_val.extend([np.asscalar(v)\r\n",
      "                                      for x in proto_values\r\n",
      "                                      for v in [x.real, x.imag]])\r\n",
      "\r\n",
      "  def SlowAppendComplex128ArrayToTensorProto(tensor_proto, proto_values):\r\n",
      "    tensor_proto.dcomplex_val.extend([np.asscalar(v)\r\n",
      "                                      for x in proto_values\r\n",
      "                                      for v in [x.real, x.imag]])\r\n",
      "\r\n",
      "  def SlowAppendObjectArrayToTensorProto(tensor_proto, proto_values):\r\n",
      "    tensor_proto.string_val.extend([compat.as_bytes(x) for x in proto_values])\r\n",
      "\r\n",
      "  def SlowAppendBoolArrayToTensorProto(tensor_proto, proto_values):\r\n",
      "    tensor_proto.bool_val.extend([np.asscalar(x) for x in proto_values])\r\n",
      "\r\n",
      "  _NP_TO_APPEND_FN = {\r\n",
      "      np.float16: SlowAppendFloat16ArrayToTensorProto,\r\n",
      "      np.float32: SlowAppendFloat32ArrayToTensorProto,\r\n",
      "      np.float64: SlowAppendFloat64ArrayToTensorProto,\r\n",
      "      np.int32: SlowAppendIntArrayToTensorProto,\r\n",
      "      np.int64: SlowAppendInt64ArrayToTensorProto,\r\n",
      "      np.uint8: SlowAppendIntArrayToTensorProto,\r\n",
      "      np.uint16: SlowAppendIntArrayToTensorProto,\r\n",
      "      np.int8: SlowAppendIntArrayToTensorProto,\r\n",
      "      np.int16: SlowAppendIntArrayToTensorProto,\r\n",
      "      np.complex64: SlowAppendComplex64ArrayToTensorProto,\r\n",
      "      np.complex128: SlowAppendComplex128ArrayToTensorProto,\r\n",
      "      np.object: SlowAppendObjectArrayToTensorProto,\r\n",
      "      np.bool: SlowAppendBoolArrayToTensorProto,\r\n",
      "      dtypes.qint8.as_numpy_dtype: SlowAppendQIntArrayToTensorProto,\r\n",
      "      dtypes.quint8.as_numpy_dtype: SlowAppendQIntArrayToTensorProto,\r\n",
      "      dtypes.qint16.as_numpy_dtype: SlowAppendQIntArrayToTensorProto,\r\n",
      "      dtypes.quint16.as_numpy_dtype: SlowAppendQIntArrayToTensorProto,\r\n",
      "      dtypes.qint32.as_numpy_dtype: SlowAppendQIntArrayToTensorProto,\r\n",
      "      # NOTE(touts): Intentionally no way to feed a DT_BFLOAT16.\r\n",
      "  }\r\n",
      "\r\n",
      "\r\n",
      "def GetFromNumpyDTypeDict(dtype_dict, dtype):\r\n",
      "  # NOTE: dtype_dict.get(dtype) always returns None.\r\n",
      "  for key, val in six.iteritems(dtype_dict):\r\n",
      "    if key == dtype:\r\n",
      "      return val\r\n",
      "  return None\r\n",
      "\r\n",
      "\r\n",
      "def GetNumpyAppendFn(dtype):\r\n",
      "  # numpy dtype for strings are variable length. We can not compare\r\n",
      "  # dtype with a single constant (np.string does not exist) to decide\r\n",
      "  # dtype is a \"string\" type. We need to compare the dtype.type to be\r\n",
      "  # sure it's a string type.\r\n",
      "  if dtype.type == np.string_ or dtype.type == np.unicode_:\r\n",
      "    if _FAST_TENSOR_UTIL_AVAILABLE:\r\n",
      "      return fast_tensor_util.AppendObjectArrayToTensorProto\r\n",
      "    else:\r\n",
      "      return SlowAppendObjectArrayToTensorProto\r\n",
      "  return GetFromNumpyDTypeDict(_NP_TO_APPEND_FN, dtype)\r\n",
      "\r\n",
      "\r\n",
      "def TensorShapeProtoToList(shape):\r\n",
      "  \"\"\"Convert a TensorShape to a list.\r\n",
      "\r\n",
      "  Args:\r\n",
      "    shape: A TensorShapeProto.\r\n",
      "\r\n",
      "  Returns:\r\n",
      "    List of integers representing the dimensions of the tensor.\r\n",
      "  \"\"\"\r\n",
      "  return [dim.size for dim in shape.dim]\r\n",
      "\r\n",
      "\r\n",
      "def _GetDenseDimensions(list_of_lists):\r\n",
      "  \"\"\"Returns the inferred dense dimensions of a list of lists.\"\"\"\r\n",
      "  if not isinstance(list_of_lists, (list, tuple)):\r\n",
      "    return []\r\n",
      "  elif not list_of_lists:\r\n",
      "    return [0]\r\n",
      "  else:\r\n",
      "    return [len(list_of_lists)] + _GetDenseDimensions(list_of_lists[0])\r\n",
      "\r\n",
      "\r\n",
      "def _FlattenToStrings(nested_strings):\r\n",
      "  if isinstance(nested_strings, (list, tuple)):\r\n",
      "    for inner in nested_strings:\r\n",
      "      for flattened_string in _FlattenToStrings(inner):\r\n",
      "        yield flattened_string\r\n",
      "  else:\r\n",
      "    yield nested_strings\r\n",
      "\r\n",
      "\r\n",
      "_TENSOR_CONTENT_TYPES = frozenset([\r\n",
      "    dtypes.float32, dtypes.float64, dtypes.int32, dtypes.uint8, dtypes.int16,\r\n",
      "    dtypes.int8, dtypes.int64, dtypes.qint8, dtypes.quint8, dtypes.qint16,\r\n",
      "    dtypes.quint16, dtypes.qint32,\r\n",
      "])\r\n",
      "\r\n",
      "\r\n",
      "class _Message(object):\r\n",
      "\r\n",
      "  def __init__(self, message):\r\n",
      "    self._message = message\r\n",
      "\r\n",
      "  def __repr__(self):\r\n",
      "    return self._message\r\n",
      "\r\n",
      "\r\n",
      "def _FirstNotNone(l):\r\n",
      "  for x in l:\r\n",
      "    if x is not None:\r\n",
      "      if isinstance(x, ops.Tensor):\r\n",
      "        return _Message(\"list containing Tensors\")\r\n",
      "      else:\r\n",
      "        return x\r\n",
      "  return None\r\n",
      "\r\n",
      "\r\n",
      "def _NotNone(v):\r\n",
      "  if v is None:\r\n",
      "    return _Message(\"None\")\r\n",
      "  else:\r\n",
      "    return v\r\n",
      "\r\n",
      "\r\n",
      "def _FilterTuple(v):\r\n",
      "  if not isinstance(v, (list, tuple)):\r\n",
      "    return v\r\n",
      "  if isinstance(v, tuple):\r\n",
      "    if not any(isinstance(x, (list, tuple)) for x in v):\r\n",
      "      return None\r\n",
      "  if isinstance(v, list):\r\n",
      "    if not any(isinstance(x, (list, tuple)) for x in v):\r\n",
      "      return _FirstNotNone([None if isinstance(x, (list, tuple)) else x for x in v])\r\n",
      "  return _FirstNotNone([_FilterTuple(x) for x in v])\r\n",
      "\r\n",
      "\r\n",
      "def _FilterInt(v):\r\n",
      "  if isinstance(v, (list, tuple)):\r\n",
      "    return _FirstNotNone([_FilterInt(x) for x in v])\r\n",
      "  return None if isinstance(v, (compat.integral_types,\r\n",
      "                                tensor_shape.Dimension)) else _NotNone(v)\r\n",
      "\r\n",
      "\r\n",
      "def _FilterFloat(v):\r\n",
      "  if isinstance(v, (list, tuple)):\r\n",
      "    return _FirstNotNone([_FilterFloat(x) for x in v])\r\n",
      "  return None if isinstance(v, compat.real_types) else _NotNone(v)\r\n",
      "\r\n",
      "\r\n",
      "def _FilterComplex(v):\r\n",
      "  if isinstance(v, (list, tuple)):\r\n",
      "    return _FirstNotNone([_FilterComplex(x) for x in v])\r\n",
      "  return None if isinstance(v, compat.complex_types) else _NotNone(v)\r\n",
      "\r\n",
      "\r\n",
      "def _FilterStr(v):\r\n",
      "  if isinstance(v, (list, tuple)):\r\n",
      "    return _FirstNotNone([_FilterStr(x) for x in v])\r\n",
      "  if isinstance(v, compat.bytes_or_text_types):\r\n",
      "    return None\r\n",
      "  else:\r\n",
      "    return _NotNone(v)\r\n",
      "\r\n",
      "\r\n",
      "def _FilterBool(v):\r\n",
      "  if isinstance(v, (list, tuple)):\r\n",
      "    return _FirstNotNone([_FilterBool(x) for x in v])\r\n",
      "  return None if isinstance(v, bool) else _NotNone(v)\r\n",
      "\r\n",
      "\r\n",
      "def _FilterNotTensor(v):\r\n",
      "  if isinstance(v, (list, tuple)):\r\n",
      "    return _FirstNotNone([_FilterNotTensor(x) for x in v])\r\n",
      "  return str(v) if isinstance(v, ops.Tensor) else None\r\n",
      "\r\n",
      "\r\n",
      "_TF_TO_IS_OK = {\r\n",
      "    dtypes.bool: [_FilterBool],\r\n",
      "    dtypes.complex128: [_FilterComplex],\r\n",
      "    dtypes.complex64: [_FilterComplex],\r\n",
      "    dtypes.float32: [_FilterFloat],\r\n",
      "    dtypes.float64: [_FilterFloat],\r\n",
      "    dtypes.int16: [_FilterInt],\r\n",
      "    dtypes.int32: [_FilterInt],\r\n",
      "    dtypes.int64: [_FilterInt],\r\n",
      "    dtypes.int8: [_FilterInt],\r\n",
      "    dtypes.qint16: [_FilterInt, _FilterTuple],\r\n",
      "    dtypes.qint32: [_FilterInt, _FilterTuple],\r\n",
      "    dtypes.qint8: [_FilterInt, _FilterTuple],\r\n",
      "    dtypes.quint16: [_FilterInt, _FilterTuple],\r\n",
      "    dtypes.quint8: [_FilterInt, _FilterTuple],\r\n",
      "    dtypes.string: [_FilterStr],\r\n",
      "    dtypes.uint16: [_FilterInt],\r\n",
      "    dtypes.uint8: [_FilterInt],\r\n",
      "}\r\n",
      "\r\n",
      "\r\n",
      "def _AssertCompatible(values, dtype):\r\n",
      "  fn_list = _TF_TO_IS_OK.get(dtype, [_FilterNotTensor])\r\n",
      "  mismatch = _FirstNotNone([fn(values) for fn in fn_list])\r\n",
      "  if mismatch is not None:\r\n",
      "    if dtype is None:\r\n",
      "      raise TypeError(\"List of Tensors when single Tensor expected\")\r\n",
      "    else:\r\n",
      "      raise TypeError(\"Expected %s, got %s of type '%s' instead.\" %\r\n",
      "                      (dtype.name, repr(mismatch), type(mismatch).__name__))\r\n",
      "\r\n",
      "\r\n",
      "def make_tensor_proto(values, dtype=None, shape=None, verify_shape=False):\r\n",
      "  \"\"\"Create a TensorProto.\r\n",
      "\r\n",
      "  Args:\r\n",
      "    values:         Values to put in the TensorProto.\r\n",
      "    dtype:          Optional tensor_pb2 DataType value.\r\n",
      "    shape:          List of integers representing the dimensions of tensor.\r\n",
      "    verify_shape:   Boolean that enables verification of a shape of values.\r\n",
      "\r\n",
      "  Returns:\r\n",
      "    A `TensorProto`. Depending on the type, it may contain data in the\r\n",
      "    \"tensor_content\" attribute, which is not directly useful to Python programs.\r\n",
      "    To access the values you should convert the proto back to a numpy ndarray\r\n",
      "    with `tensor_util.MakeNdarray(proto)`.\r\n",
      "\r\n",
      "    If `values` is a `TensorProto`, it is immediately returned; `dtype` and\r\n",
      "    `shape` are ignored.\r\n",
      "\r\n",
      "  Raises:\r\n",
      "    TypeError:  if unsupported types are provided.\r\n",
      "    ValueError: if arguments have inappropriate values or if verify_shape is\r\n",
      "     True and shape of values is not equals to a shape from the argument.\r\n",
      "\r\n",
      "  make_tensor_proto accepts \"values\" of a python scalar, a python list, a\r\n",
      "  numpy ndarray, or a numpy scalar.\r\n",
      "\r\n",
      "  If \"values\" is a python scalar or a python list, make_tensor_proto\r\n",
      "  first convert it to numpy ndarray. If dtype is None, the\r\n",
      "  conversion tries its best to infer the right numpy data\r\n",
      "  type. Otherwise, the resulting numpy array has a compatible data\r\n",
      "  type with the given dtype.\r\n",
      "\r\n",
      "  In either case above, the numpy ndarray (either the caller provided\r\n",
      "  or the auto converted) must have the compatible type with dtype.\r\n",
      "\r\n",
      "  make_tensor_proto then converts the numpy array to a tensor proto.\r\n",
      "\r\n",
      "  If \"shape\" is None, the resulting tensor proto represents the numpy\r\n",
      "  array precisely.\r\n",
      "\r\n",
      "  Otherwise, \"shape\" specifies the tensor's shape and the numpy array\r\n",
      "  can not have more elements than what \"shape\" specifies.\r\n",
      "\r\n",
      "  \"\"\"\r\n",
      "  if isinstance(values, tensor_pb2.TensorProto):\r\n",
      "    return values\r\n",
      "\r\n",
      "  if dtype:\r\n",
      "    dtype = dtypes.as_dtype(dtype)\r\n",
      "\r\n",
      "  is_quantized = (dtype in [dtypes.qint8, dtypes.quint8, dtypes.qint16,\r\n",
      "                            dtypes.quint16, dtypes.qint32])\r\n",
      "\r\n",
      "  # We first convert value to a numpy array or scalar.\r\n",
      "  if isinstance(values, (np.ndarray, np.generic)):\r\n",
      "    if dtype:\r\n",
      "      nparray = values.astype(dtype.as_numpy_dtype)\r\n",
      "    else:\r\n",
      "      nparray = values\r\n",
      "  elif callable(getattr(values, \"__array__\", None)):\r\n",
      "    # If a class has the __array__ method, then it is possible to convert\r\n",
      "    # to numpy array.\r\n",
      "    nparray = np.asarray(values, dtype=dtype)\r\n",
      "  else:\r\n",
      "    if values is None:\r\n",
      "      raise ValueError(\"None values not supported.\")\r\n",
      "    # if dtype is provided, forces numpy array to be the type\r\n",
      "    # provided if possible.\r\n",
      "    if dtype and dtype.is_numpy_compatible:\r\n",
      "      np_dt = dtype.as_numpy_dtype\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 375 /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/tensor_util.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "verbose = 0\n",
    "sim_gen = simulator_generator_example(max_ip_seq_length,max_seq_communication_length)\n",
    "num_epoch_steps=1\n",
    "\n",
    "\n",
    "for epoch_num in range(0,num_epoch,num_epoch_steps):\n",
    "    batch = sim_gen.__next__()\n",
    "    \n",
    "    \n",
    "    encoder_output = encoder_agent.predict(batch)\n",
    "    encoder_output_disctretised = np.argmax(encoder_output,axis=-1)\n",
    "    encoder_output_disctretised=np.expand_dims(encoder_output_disctretised,axis=-1)\n",
    "    \n",
    "    \n",
    "    dummy_decoder_input = np.zeros((encoder_output_disctretised.shape[0],max_op_seq_length,1))\n",
    "    decoder_output=decoder_agent.predict([encoder_output_disctretised,dummy_decoder_input])\n",
    "\n",
    "    decoder_output_discrete = np.argmax(decoder_output,axis=-1)\n",
    "    \n",
    "    print('encoder_output:',encoder_output_disctretised[0].flatten())\n",
    "    print('decoder_output:',decoder_output_discrete[0])\n",
    "    \n",
    "    rewards = []\n",
    "    for idx,(ip_exp,op_exp) in enumerate(zip(batch[0],decoder_output_discrete)):\n",
    "        try:\n",
    "            ip_exp=np.squeeze(ip_exp,axis=-1)\n",
    "            #print('ip_exp:',ip_exp)\n",
    "            reward = score_function(ip_exp,op_exp)\n",
    "            if reward:\n",
    "                reward = 1\n",
    "            else:\n",
    "                reward = -1\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            reward = -1\n",
    "        rewards.append(reward)\n",
    "    \n",
    "    #print('input:',ip_exp)\n",
    "    #print('output:',op_exp)\n",
    "    \n",
    "    \n",
    "    print(np.sum(rewards),np.mean(rewards))\n",
    "    encoder_reward = sparse_to_dense(encoder_output_disctretised,rewards,sequential_code_channel_size)\n",
    "    decoder_reward = sparse_to_dense(np.expand_dims(decoder_output_discrete,axis=-1),rewards,num_vocab)\n",
    "    \n",
    "    \n",
    "    #import pdb; pdb.set_trace()\n",
    "    \n",
    "    encoder_agent.fit(batch,encoder_reward,\n",
    "                      initial_epoch = num_epoch,\n",
    "                      epochs=num_epoch+num_epoch_steps,\n",
    "                      #callbacks=[tb_callback],\n",
    "                      verbose=verbose)\n",
    "    decoder_agent.fit([encoder_output_disctretised,dummy_decoder_input],decoder_reward,\n",
    "                      initial_epoch = num_epoch,\n",
    "                      epochs=num_epoch+num_epoch_steps,\n",
    "                      #callbacks=[tb_callback],\n",
    "                      verbose=verbose)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#     encoder_seq_gen_inp_initial=np.zeros((batch_size,1,1))\n",
    "#     encoder_seq_gen_inp = encoder_seq_gen_inp_initial.copy()\n",
    "#     for p in range(0,max_seq_communication_length):   \n",
    "#         #print('encoder_seq_gen_inp:',encoder_seq_gen_inp.shape)\n",
    "#         encoder_output = encoder_agent.predict([encoder_inp,encoder_seq_gen_inp])\n",
    "#         encoder_output_disctretised = np.argmax(encoder_output,axis=-1)\n",
    "#         encoder_output_disctretised=np.expand_dims(encoder_output_disctretised,axis=-1)\n",
    "#         encoder_seq_gen_inp = np.concatenate((encoder_seq_gen_inp_initial, encoder_output_disctretised), axis=-2)\n",
    "#     encoder_seq_gen_inp=encoder_seq_gen_inp[:,:-1,:]\n",
    "    \n",
    "    encoder_seq_gen_inp,encoder_output_disctretised=predict_each_step(encoder_agent,encoder_inp,max_seq_communication_length)\n",
    "    decoder_seq_gen_inp,decoder_output_disctretised=predict_each_step(decoder_agent,encoder_output_disctretised,max_op_seq_length)\n",
    "    \n",
    "    \n",
    "#     decoder_seq_gen_inp_initial=np.zeros((batch_size,1,1))\n",
    "#     decoder_seq_gen_inp = decoder_seq_gen_inp_initial.copy()\n",
    "#     for p in range(0,max_op_seq_length):    \n",
    "#         #print('decoder_seq_gen_inp:',decoder_seq_gen_inp.shape)\n",
    "#         decoder_output=decoder_agent.predict([encoder_output_disctretised,decoder_seq_gen_inp])\n",
    "#         decoder_output_disctretised = np.argmax(decoder_output,axis=-1)\n",
    "#         decoder_output_disctretised=np.expand_dims(decoder_output_disctretised,axis=-1)\n",
    "#         decoder_seq_gen_inp = np.concatenate((decoder_seq_gen_inp_initial, decoder_output_disctretised), axis=-2)\n",
    "#     decoder_seq_gen_inp=decoder_seq_gen_inp[:,:-1,:]\n",
    "    \n",
    "    #print('encoder_input:',encoder_inp[0].flatten())\n",
    "    #print('encoder_seq_gen_inp:',encoder_seq_gen_inp[0].flatten())\n",
    "    #print('encoder_output:',encoder_output_disctretised[0].flatten())\n",
    "    #print('decoder_output:',decoder_output_disctretised[0].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = np.asarray([[3],[3],[3]])\n",
    "b = [0,0.1,0.05]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray([z*u for z,u in zip(a,b)]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
